{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "use_Yangsan_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxyO2qsPR6aMEgMrF849fG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devmoon0/Best-README-Template/blob/master/use_Yangsan_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 양산시 데이터셋 사용\n",
        "- 기존형태 :  마이크로 소프트사의 VOTT 프로그램을 사용하여 Jpg이미지와 Json 파일 형식\n",
        "- 원하는 형태 : yolov5\n",
        "\n",
        "##1.1 변환방법\n",
        "- vott -> coco -> yolov5"
      ],
      "metadata": {
        "id": "qlsLQSLcn7Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive 접근을 위한 Mount 적용. \n",
        "import os, sys \n",
        "from google.colab import drive \n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "# soft link로 Google Drive Directory 연결. \n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc8_BBx7pN1H",
        "outputId": "7a8a1653-ce5a-49e5-d4f0-5fcb0b5bd0e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /mydrive/yangsan_dataset\n",
        "!cp /mydrive/yangsan_dataset/UAVVaste_coco.zip /content/dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2tzDjTopW9h",
        "outputId": "9f216c62-533e-499f-e9fa-41e8f30b7329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UAVVaste_coco.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/dataset\n",
        "!unzip -qq  \"/content/dataset/UAVVaste_coco.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsybRCLAs0CJ",
        "outputId": "09701736-6f33-4b7e-ad97-765e147aab0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 coco -> yolo"
      ],
      "metadata": {
        "id": "sRPRIQAEtdoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /mydrive/yangsan_dataset\n",
        "!cp /mydrive/yangsan_dataset/annotations.json /content/dataset/yansan_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLlYYPgyOAKP",
        "outputId": "082ed497-71fe-4d33-d846-2d65e47cd81b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "annotations.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/dataset/bccd;\n",
        "!cd /content/dataset/bccd; mkdir images; mkdir labels;"
      ],
      "metadata": {
        "id": "upee-9c-thYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/alexmihalyk23/COCO2YOLO.git\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "class COCO2YOLO:\n",
        "  # 소스 이미지 디렉토리와 Json annotation 파일, 타겟 이미지 디렉토리, 타겟 annotation 디렉토리를 생성자로 입력 받음. \n",
        "  def __init__(self, src_img_dir, json_file, tgt_img_dir, tgt_anno_dir):\n",
        "    self.json_file = json_file\n",
        "    self.src_img_dir = src_img_dir\n",
        "    self.tgt_img_dir = tgt_img_dir\n",
        "    self.tgt_anno_dir = tgt_anno_dir\n",
        "    # json 파일과 타겟 디렉토리가 존재하는지 확인하고, 디렉토리의 경우는 없으면 생성. \n",
        "    self._check_file_and_dir(json_file, tgt_img_dir, tgt_anno_dir)\n",
        "    # json 파일을 메모리로 로딩. \n",
        "    self.labels = json.load(open(json_file, 'r', encoding='utf-8'))\n",
        "    # category id와 이름을 매핑하지만, 실제 class id는 이를 적용하지 않고 별도 적용. \n",
        "    self.coco_id_name_map = self._categories()\n",
        "    self.coco_name_list = list(self.coco_id_name_map.values())\n",
        "    print(\"total images\", len(self.labels['images']))\n",
        "    print(\"total categories\", len(self.labels['categories']))\n",
        "    print(\"total labels\", len(self.labels['annotations']))\n",
        "  \n",
        "  # json 파일과 타겟 디렉토리가 존재하는지 확인하고, 디렉토리의 경우는 없으면 생성. \n",
        "  def _check_file_and_dir(self, file_path, tgt_img_dir, tgt_anno_dir):\n",
        "    if not os.path.exists(file_path):\n",
        "        raise ValueError(\"file not found\")\n",
        "    if not os.path.exists(tgt_img_dir):\n",
        "        os.makedirs(tgt_img_dir)\n",
        "    if not os.path.exists(tgt_anno_dir):\n",
        "        os.makedirs(tgt_anno_dir)\n",
        "\n",
        "  # category id와 이름을 매핑하지만, 추후에 class 명만 활용. \n",
        "  def _categories(self):\n",
        "    categories = {}\n",
        "    for cls in self.labels['categories']:\n",
        "        categories[cls['id']] = cls['name']\n",
        "    return categories\n",
        "  \n",
        "  # annotation에서 모든 image의 파일명(절대 경로 아님)과 width, height 정보 저장. \n",
        "  def _load_images_info(self):\n",
        "    images_info = {}\n",
        "    for image in self.labels['images']:\n",
        "        id = image['id']\n",
        "        file_name = image['file_name']\n",
        "        if file_name.find('\\\\') > -1:\n",
        "            file_name = file_name[file_name.index('\\\\')+1:]\n",
        "        w = image['width']\n",
        "        h = image['height']\n",
        "  \n",
        "        images_info[id] = (file_name, w, h)\n",
        "\n",
        "    return images_info\n",
        "\n",
        "  # ms-coco의 bbox annotation은 yolo format으로 변환. 좌상단 x, y좌표, width, height 기반을 정규화된 center x,y 와 width, height로 변환. \n",
        "  def _bbox_2_yolo(self, bbox, img_w, img_h):\n",
        "    # ms-coco는 좌상단 x, y좌표, width, height\n",
        "    x, y, w, h = bbox[0], bbox[1], bbox[2], bbox[3]\n",
        "    # center x좌표는 좌상단 x좌표에서 width의 절반을 더함. center y좌표는 좌상단 y좌표에서 height의 절반을 더함.  \n",
        "    centerx = bbox[0] + w / 2\n",
        "    centery = bbox[1] + h / 2\n",
        "    # centerx, centery, width, height를 이미지의 width/height로 정규화. \n",
        "    dw = 1 / img_w\n",
        "    dh = 1 / img_h\n",
        "    centerx *= dw\n",
        "    w *= dw\n",
        "    centery *= dh\n",
        "    h *= dh\n",
        "    return centerx, centery, w, h\n",
        "  \n",
        "  # image와 annotation 정보를 기반으로 image명과 yolo annotation 정보 가공. \n",
        "  # 개별 image당 하나의 annotation 정보를 가지도록 변환. \n",
        "  def _convert_anno(self, images_info):\n",
        "    anno_dict = dict()\n",
        "    for anno in self.labels['annotations']:\n",
        "      bbox = anno['bbox']\n",
        "      image_id = anno['image_id']\n",
        "      category_id = anno['category_id']\n",
        "\n",
        "      image_info = images_info.get(image_id)\n",
        "      image_name = image_info[0]\n",
        "      img_w = image_info[1]\n",
        "      img_h = image_info[2]\n",
        "      yolo_box = self._bbox_2_yolo(bbox, img_w, img_h)\n",
        "\n",
        "      anno_info = (image_name, category_id, yolo_box)\n",
        "      anno_infos = anno_dict.get(image_id)\n",
        "      if not anno_infos:\n",
        "        anno_dict[image_id] = [anno_info]\n",
        "      else:\n",
        "        anno_infos.append(anno_info)\n",
        "        anno_dict[image_id] = anno_infos\n",
        "    return anno_dict\n",
        "\n",
        "  # class 명을 파일로 저장하는 로직. 사용하지 않음. \n",
        "  def save_classes(self):\n",
        "    sorted_classes = list(map(lambda x: x['name'], sorted(self.labels['categories'], key=lambda x: x['id'])))\n",
        "    print('coco names', sorted_classes)\n",
        "    with open('coco.names', 'w', encoding='utf-8') as f:\n",
        "      for cls in sorted_classes:\n",
        "          f.write(cls + '\\n')\n",
        "    f.close()\n",
        "  # _convert_anno(images_info)로 만들어진 anno 정보를 개별 yolo anno txt 파일로 생성하는 로직. \n",
        "  # coco2yolo()에서 anno_dict = self._convert_anno(images_info)로 만들어진 anno_dict를 _save_txt()에 입력하여 파일 생성\n",
        "  def _save_txt(self, anno_dict):\n",
        "    # 개별 image별로 소스 image는 타겟이미지 디렉토리로 복사하고, 개별 annotation을 타겟 anno 디렉토리로 생성. \n",
        "    for k, v in anno_dict.items():\n",
        "      # 소스와 타겟 파일의 절대 경로 생성. \n",
        "      src_img_filename = os.path.join(self.src_img_dir, v[0][0])\n",
        "      tgt_anno_filename = os.path.join(self.tgt_anno_dir,v[0][0].split(\".\")[0] + \".txt\")\n",
        "      #print('source image filename:', src_img_filename, 'target anno filename:', tgt_anno_filename)\n",
        "      # 이미지 파일의 경우 타겟 디렉토리로 단순 복사. \n",
        "      shutil.copy(src_img_filename, self.tgt_img_dir)\n",
        "      # 타겟 annotation 출력 파일명으로 classid, bbox 좌표를 object 별로 생성. \n",
        "      with open(tgt_anno_filename, 'w', encoding='utf-8') as f:\n",
        "        #print(k, v)\n",
        "        # 여러개의 object 별로 classid와 bbox 좌표를 생성. \n",
        "        for obj in v:\n",
        "          cat_name = self.coco_id_name_map.get(obj[1])\n",
        "          # category_id는 class 명에 따라 0부터 순차적으로 부여. \n",
        "          category_id = self.coco_name_list.index(cat_name)\n",
        "          #print('cat_name:', cat_name, 'category_id:', category_id)\n",
        "        #   box = ['{:.6f}'.format(x) for x in obj[2]]\n",
        "        #   box = ' '.join(box)\n",
        "        #   line = str(category_id) + ' ' + box\n",
        "        #   f.write(line + '\\n')\n",
        "\n",
        "  # ms-coco를 yolo format으로 변환. \n",
        "  def coco2yolo(self):\n",
        "    print(\"loading image info...\")\n",
        "    images_info = self._load_images_info()\n",
        "    print(\"loading done, total images\", len(images_info))\n",
        "\n",
        "    print(\"start converting...\")\n",
        "    anno_dict = self._convert_anno(images_info)\n",
        "    print(\"converting done, total labels\", len(anno_dict))\n",
        "\n",
        "    print(\"saving txt file...\")\n",
        "    self._save_txt(anno_dict)\n",
        "    print(\"saving done\")\n"
      ],
      "metadata": {
        "id": "HZ_8EQExt7RZ"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습/검증/테스트용 images, labels 디렉토리 생성. \n",
        "!mkdir /content/bccd;\n",
        "!cd /content/bccd; mkdir images; mkdir labels;\n",
        "!cd /content/bccd/images; mkdir train; mkdir val; mkdir test\n",
        "!cd /content/bccd/labels; mkdir train; mkdir val; mkdir test"
      ],
      "metadata": {
        "id": "wsi0KbX_TtUd"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train 용 yolo 데이터 세트 생성. \n",
        "train_yolo_converter = COCO2YOLO(src_img_dir='/content/dataset/yansan_dataset/images', json_file='/content/dataset/yansan_dataset/annotations.json',\n",
        "                                 tgt_img_dir='/content/bccd/images/train', tgt_anno_dir='/content/bccd/labels/train')\n",
        "train_yolo_converter.coco2yolo()\n",
        "\n",
        "# val 용 yolo 데이터 세트 생성. \n",
        "val_yolo_converter = COCO2YOLO(src_img_dir='/content/dataset/yansan_dataset/images', json_file='/content/dataset/yansan_dataset/annotations.json',\n",
        "                                 tgt_img_dir='/content/bccd/images/val', tgt_anno_dir='/content/bccd/labels/val')\n",
        "val_yolo_converter.coco2yolo()\n",
        "\n",
        "# test 용 yolo 데이터 세트 생성. \n",
        "test_yolo_converter = COCO2YOLO(src_img_dir='/content/dataset/yansan_dataset/images', json_file='/content/dataset/yansan_dataset/annotations.json',\n",
        "                                 tgt_img_dir='/content/bccd/images/test', tgt_anno_dir='/content/bccd/labels/test')\n",
        "test_yolo_converter.coco2yolo()\n",
        "\n",
        "# test_yolo_converter = COCO2YOLO(src_img_dir='/content/dataset/yansan_dataset/images', \n",
        "#                                 json_file='/content/dataset/yansan_dataset/annotations.json',\n",
        "#                                 tgt_img_dir='/content/dataset/bccd/images', \n",
        "#                                 tgt_anno_dir='/content/dataset/bccd/labels')\n",
        "# test_yolo_converter.coco2yolo()"
      ],
      "metadata": {
        "id": "nz7ern9Kt9HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLOv5환경"
      ],
      "metadata": {
        "id": "mdhNQ3CH9DcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qr requirements.txt "
      ],
      "metadata": {
        "id": "b3BUwY7WPAsY"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "!cd yolov5;pip install -qr requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uXFUAmHu69z",
        "outputId": "78d68493-d53d-4a89-809e-07d97343c337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 11301, done.\u001b[K\n",
            "remote: Total 11301 (delta 0), reused 0 (delta 0), pack-reused 11301\u001b[K\n",
            "Receiving objects: 100% (11301/11301), 11.20 MiB | 26.79 MiB/s, done.\n",
            "Resolving deltas: 100% (7810/7810), done.\n",
            "\u001b[K     |████████████████████████████████| 596 kB 3.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov5\n",
        "%pip install -qr requirements.txt "
      ],
      "metadata": {
        "id": "OfcqTb1jAp0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터셋 분할 - 방법 (1)"
      ],
      "metadata": {
        "id": "9QF8nboPBL6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "\n",
        "img_list = glob('/content/dataset/bccd/images/*.jpg')\n",
        "print(len(img_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOQ-d-2tBQDe",
        "outputId": "8e9f234f-0ed7-4dec-e5cf-636c0f2e84be"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_img_list, val_img_list = train_test_split(img_list, test_size=0.2, random_state=2000)\n",
        "\n",
        "print(len(train_img_list), len(val_img_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdqJB2zQBW7J",
        "outputId": "b1000d94-933b-45fd-fc70-3f4867b5c2e6"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3257 815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/dataset/train.txt', 'w') as f:\n",
        "  f.write('\\n'.join(train_img_list) + '\\n')\n",
        "\n",
        "with open('/content/dataset/val.txt', 'w') as f:\n",
        "  f.write('\\n'.join(val_img_list) + '\\n')"
      ],
      "metadata": {
        "id": "MIQLS_0NBaE9"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "with open('/content/dataset/data.yaml', 'r') as f:\n",
        "  data = yaml.load(f)\n",
        "\n",
        "print(data)\n",
        "\n",
        "data['train'] = '/content/dataset/train.txt'\n",
        "data['val'] = '/content/dataset/val.txt'\n",
        "\n",
        "with open('/content/dataset/data.yaml', 'w') as f:\n",
        "  yaml.dump(data, f)\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "X0fQWt0SBdoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/yolov5/\n",
        "\n",
        "!python train.py --img 640 --batch 8 --epochs 30  --data /content/dataset/data.yaml --weights yolov5l.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhjkzt5PF8lm",
        "outputId": "ea62784e-e29d-4516-f7fe-3831f138ad81"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5l.pt, cfg=, data=/content/dataset/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=30, batch_size=8, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v6.1-35-g932dc78 torch 1.10.0+cu111 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \n",
            "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \n",
            "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \n",
            "  7                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
            "  8                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \n",
            "  9                -1  1   2624512  models.common.SPPF                      [1024, 1024, 5]               \n",
            " 10                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \n",
            " 14                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \n",
            " 18                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \n",
            " 21                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  3   9971712  models.common.C3                        [1024, 1024, 3, False]        \n",
            " 24      [17, 20, 23]  1     70005  models.yolo.Detect                      [8, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\n",
            "Model Summary: 468 layers, 46175989 parameters, 46175989 gradients, 108.0 GFLOPs\n",
            "\n",
            "Transferred 607/613 items from yolov5l.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 101 weight (no decay), 104 weight, 104 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/bccd/labels/train' images and labels...4072 found, 0 missing, 4072 empty, 0 corrupt: 100% 4072/4072 [00:01<00:00, 2414.57it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/bccd/labels/train.cache\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 643, in <module>\n",
            "    main(opt)\n",
            "  File \"train.py\", line 539, in main\n",
            "    train(opt.hyp, opt, device, callbacks)\n",
            "  File \"train.py\", line 228, in train\n",
            "    mlc = int(np.concatenate(dataset.labels, 0)[:, 0].max())  # max label class\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py\", line 40, in _amax\n",
            "    return umr_maximum(a, axis, None, out, keepdims, initial, where)\n",
            "ValueError: zero-size array to reduction operation maximum which has no identity\n"
          ]
        }
      ]
    }
  ]
}